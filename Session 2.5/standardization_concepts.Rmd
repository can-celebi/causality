---
title: "Understanding Standardization and SMD"
author: "Causal Inference Course"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 3
    theme: flatly
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, fig.width = 8, fig.height = 5)
library(ggplot2)
library(knitr)
set.seed(123)
```

# Introduction

This tutorial explains standardization from first principles, building up to the Standardized Mean Difference (SMD) used in propensity score matching.

## Topics Covered

1.  What is standardization? (z-scores)
2.  Manual calculation with for loops
3.  Using built-in functions (scale())
4.  Why standardize? (comparing apples to oranges)
5.  Pooled standard deviation
6.  Standardized Mean Difference (SMD)
7.  SMD vs Statistical Significance

------------------------------------------------------------------------

# Part 1: What is Standardization?

## 1.1 The Problem: Variables Have Different Scales

Imagine we have data on people with very different measurement scales:

```{r different-scales}
age <- c(25, 30, 35, 40, 45, 50, 55, 60)       # Age in years (range: 25-60)
income <- c(30, 45, 55, 70, 80, 95, 110, 150)  # Income in thousands (range: 30-150)
```

| Variable           | Values                             |
|--------------------|------------------------------------|
| Age (years)        | `r paste(age, collapse = ", ")`    |
| Income (thousands) | `r paste(income, collapse = ", ")` |

**The Problem:**

-   Age ranges from 25 to 60 (spread of 35)
-   Income ranges from 30 to 150 (spread of 120)
-   Is a difference of 10 years "the same" as \$10k?
-   **We need a common scale!**

## 1.2 The Solution: Z-scores (Standardization)

Standardization converts any variable to a common scale:

-   Mean becomes **0**
-   Standard deviation becomes **1**
-   Values are now in "standard deviation units"

### The Formula

$$z = \frac{\text{value} - \text{mean}}{\text{standard deviation}}$$

### Interpretation of Z-scores

| Z-score | Interpretation                                |
|---------|-----------------------------------------------|
| z = 0   | Exactly average                               |
| z = 1   | 1 standard deviation above average            |
| z = -1  | 1 standard deviation below average            |
| z = 2   | 2 standard deviations above average (unusual) |
| z = -2  | 2 standard deviations below average (unusual) |

------------------------------------------------------------------------

# Part 2: Manual Calculation with For Loops

## 2.1 Step 1: Calculate the Mean

```{r manual-mean}
n <- length(age)
sumAge <- 0

for (i in 1:n) {
  sumAge <- sumAge + age[i]
}

meanAge <- sumAge / n
```

Calculating mean of age manually:

-   Sum of all values: `r sumAge`
-   Number of values: `r n`
-   Mean = Sum / N = `r sumAge` / `r n` = **`r meanAge`**

Verification with `mean()`: `r mean(age)` ✓

## 2.2 Step 2: Calculate the Standard Deviation

Standard deviation measures how spread out the data is.

$$SD = \sqrt{\frac{\sum(x - \text{mean})^2}{n - 1}}$$

```{r manual-sd}
sumSquaredDev <- 0

cat("Calculating squared deviations from mean:\n")
for (i in 1:n) {
  deviation <- age[i] - meanAge
  squaredDev <- deviation^2
  sumSquaredDev <- sumSquaredDev + squaredDev

  cat("  age[", i, "] =", age[i],
      ", deviation =", round(deviation, 2),
      ", squared =", round(squaredDev, 2), "\n")
}

variance <- sumSquaredDev / (n - 1)  # Note: n-1 for sample variance
sdAge <- sqrt(variance)
```

-   Sum of squared deviations: `r round(sumSquaredDev, 2)`
-   Variance = Sum / (n-1) = `r round(sumSquaredDev, 2)` / `r n-1` = `r round(variance, 2)`
-   SD = sqrt(Variance) = sqrt(`r round(variance, 2)`) = **`r round(sdAge, 2)`**

Verification with `sd()`: `r round(sd(age), 2)` ✓

## 2.3 Step 3: Calculate Z-scores for Each Value

```{r manual-zscore}
zAge <- numeric(n)

cat("Applying z-score formula: z = (value - mean) / SD\n")
cat("Mean =", meanAge, ", SD =", round(sdAge, 2), "\n\n")

for (i in 1:n) {
  zAge[i] <- (age[i] - meanAge) / sdAge

  cat("  age[", i, "] =", age[i],
      ": z = (", age[i], "-", meanAge, ") /", round(sdAge, 2),
      "=", round(zAge[i], 3), "\n")
}
```

| Original ages    | `r paste(age, collapse = ", ")`            |
|------------------|--------------------------------------------|
| Standardized (z) | `r paste(round(zAge, 3), collapse = ", ")` |

### Verifying Z-score Properties

```{r verify-zscore}
mean_z <- mean(zAge)
sd_z <- sd(zAge)
```

-   Mean of z-scores: `r round(mean_z, 10)` (should be \~0) ✓
-   SD of z-scores: `r round(sd_z, 10)` (should be \~1) ✓

------------------------------------------------------------------------

# Part 3: Using Built-in Functions

## 3.1 The `scale()` Function

R's `scale()` function does standardization automatically:

```{r scale-function}
zAgeBuiltin <- scale(age)
```

| Method             | Result                                            |
|--------------------|---------------------------------------------------|
| Using `scale(age)` | `r paste(round(zAgeBuiltin, 3), collapse = ", ")` |
| Manual calculation | `r paste(round(zAge, 3), collapse = ", ")`        |

Maximum difference: `r max(abs(zAge - as.vector(zAgeBuiltin)))` ✓

## 3.2 Standardizing Multiple Variables

```{r standardize-multiple}
df <- data.frame(age = age, income = income)

# Standardize both columns
dfStd <- as.data.frame(scale(df))
```

**Original data:**

```{r echo=FALSE}
knitr::kable(df)
```

**Standardized data:**

```{r echo=FALSE}
knitr::kable(round(dfStd, 3))
```

Now both variables are on the same scale! We can directly compare: a z-score of 1.5 means the same thing for both age and income (1.5 SDs above average).

------------------------------------------------------------------------

# Part 4: Why Standardize? (Visual Demonstration)

```{r generate-demo-data}
set.seed(456)
n_large <- 200

# Generate data with very different scales
height_cm <- rnorm(n_large, mean = 170, sd = 10)    # Height: 150-190 cm
weight_kg <- rnorm(n_large, mean = 70, sd = 15)     # Weight: 40-100 kg
salary_k <- rnorm(n_large, mean = 50, sd = 20)      # Salary: 10-90 thousand
```

## Before Standardization

```{r plot-before, fig.height=4}
plotDataBefore <- data.frame(
  value = c(height_cm, weight_kg, salary_k),
  variable = rep(c("Height (cm)", "Weight (kg)", "Salary ($k)"), each = n_large)
)

ggplot(plotDataBefore, aes(x = variable, y = value, fill = variable)) +
  geom_boxplot(alpha = 0.7) +
  labs(title = "BEFORE Standardization",
       subtitle = "Variables have completely different scales - hard to compare",
       x = "", y = "Value (original units)") +
  theme_minimal() +
  theme(legend.position = "none")
```

## After Standardization

```{r plot-after, fig.height=4}
height_z <- scale(height_cm)
weight_z <- scale(weight_kg)
salary_z <- scale(salary_k)

plotDataAfter <- data.frame(
  value = c(height_z, weight_z, salary_z),
  variable = rep(c("Height (z)", "Weight (z)", "Salary (z)"), each = n_large)
)

ggplot(plotDataAfter, aes(x = variable, y = value, fill = variable)) +
  geom_boxplot(alpha = 0.7) +
  labs(title = "AFTER Standardization",
       subtitle = "All variables now on same scale (standard deviations from mean)",
       x = "", y = "Z-score (standard deviations)") +
  theme_minimal() +
  theme(legend.position = "none") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red")
```

**After standardization:**

-   All variables have mean = 0 and SD = 1
-   We can now compare "apples to oranges"
-   A person with z = 2 for height is equally unusual as z = 2 for salary

------------------------------------------------------------------------

# Part 5: Pooled Standard Deviation

## 5.1 The Problem: Two Groups with Different SDs

```{r two-groups}
set.seed(789)
group1 <- rnorm(50, mean = 100, sd = 10)  # Less variable
group2 <- rnorm(50, mean = 110, sd = 20)  # More variable
```

| Group    | Mean                       | SD                       |
|----------|----------------------------|--------------------------|
| School 1 | `r round(mean(group1), 2)` | `r round(sd(group1), 2)` |
| School 2 | `r round(mean(group2), 2)` | `r round(sd(group2), 2)` |

**Question:** How different are these groups?

**Problem:** They have different SDs. Which SD should we use to standardize?

## 5.2 Solution: Pooled Standard Deviation

Pooled SD combines the variability from both groups. It's like asking: "What's the typical spread across both groups combined?"

### Formula (equal group sizes)

$$\text{Pooled SD} = \sqrt{\frac{SD_1^2 + SD_2^2}{2}}$$

```{r pooled-sd}
sd1 <- sd(group1)
sd2 <- sd(group2)

pooledSD <- sqrt((sd1^2 + sd2^2) / 2)
```

| Component            | Value                             |
|----------------------|-----------------------------------|
| SD₁                  | `r round(sd1, 2)`                 |
| SD₁²                 | `r round(sd1^2, 2)`               |
| SD₂                  | `r round(sd2, 2)`                 |
| SD₂²                 | `r round(sd2^2, 2)`               |
| Average of variances | `r round((sd1^2 + sd2^2) / 2, 2)` |
| **Pooled SD**        | **`r round(pooledSD, 2)`**        |

Note: Pooled SD (`r round(pooledSD, 2)`) is between SD₁ (`r round(sd1, 2)`) and SD₂ (`r round(sd2, 2)`)

## 5.3 Why Not Just Average the SDs?

```{r simple-avg}
simpleAvg <- (sd1 + sd2) / 2
```

| Method                          | Value                   |
|---------------------------------|-------------------------|
| Simple average: (SD₁ + SD₂) / 2 | `r round(simpleAvg, 2)` |
| Pooled SD                       | `r round(pooledSD, 2)`  |

They're different! Why use pooled SD?

-   **Variance (SD²) is additive**, SD is not
-   Pooled SD properly accounts for the spread in each group
-   It's the mathematically correct way to combine variability

------------------------------------------------------------------------

# Part 6: Standardized Mean Difference (SMD)

## 6.1 What is SMD?

SMD measures how different two groups are, **in standard deviation units**.

### Formula

$$SMD = \frac{\text{Mean(Group 1)} - \text{Mean(Group 2)}}{\text{Pooled SD}}$$

### Interpretation

| SMD | Interpretation                             |
|-----|--------------------------------------------|
| 0   | Groups have identical means                |
| 0.2 | Small difference (0.2 SDs apart)           |
| 0.5 | Medium difference (0.5 SDs apart)          |
| 0.8 | Large difference (0.8 SDs apart)           |
| 1.0 | Very large (the means are 1 full SD apart) |

## 6.2 Manual SMD Calculation

```{r manual-smd}
mean1 <- mean(group1)
mean2 <- mean(group2)
meanDiff <- mean1 - mean2

smd <- meanDiff / pooledSD
```

Using our school example:

|                | Value                  |
|----------------|------------------------|
| Mean(School 1) | `r round(mean1, 2)`    |
| Mean(School 2) | `r round(mean2, 2)`    |
| Difference     | `r round(meanDiff, 2)` |
| Pooled SD      | `r round(pooledSD, 2)` |
| **SMD**        | **`r round(smd, 3)`**  |

Interpretation: The schools differ by **`r round(abs(smd), 2)` standard deviations**.

```{r interpret-smd, echo=FALSE}
if (abs(smd) < 0.2) {
  cat("This is a SMALL difference.")
} else if (abs(smd) < 0.5) {
  cat("This is a SMALL-to-MEDIUM difference.")
} else if (abs(smd) < 0.8) {
  cat("This is a MEDIUM difference.")
} else {
  cat("This is a LARGE difference.")
}
```

## 6.3 Creating a Reusable SMD Function

```{r smd-function}
calcSMD <- function(x, groupIndicator) {
  # x: the variable to compare
  # groupIndicator: 0/1 indicator for which group each observation belongs to

  # Step 1: Calculate means for each group
  mean1 <- mean(x[groupIndicator == 1])
  mean0 <- mean(x[groupIndicator == 0])

  # Step 2: Calculate SDs for each group
  sd1 <- sd(x[groupIndicator == 1])
  sd0 <- sd(x[groupIndicator == 0])

  # Step 3: Calculate pooled SD
  pooledSD <- sqrt((sd1^2 + sd0^2) / 2)

  # Step 4: Calculate SMD
  smd <- (mean1 - mean0) / pooledSD

  return(smd)
}
```

## 6.4 SMD in Propensity Score Matching

```{r psm-example}
set.seed(123)
n_obs <- 200

# Confounded data: treatment depends on age
age_sim <- rnorm(n_obs, mean = 40, sd = 10)
treat_prob <- plogis(-2 + 0.05 * age_sim)  # Higher age -> more likely treated
treatment <- rbinom(n_obs, 1, treat_prob)

smd_age <- calcSMD(age_sim, treatment)
```

In propensity score matching, we use SMD to check **BALANCE**.

| Group   | Mean Age                                    |
|---------|---------------------------------------------|
| Treated | `r round(mean(age_sim[treatment == 1]), 2)` |
| Control | `r round(mean(age_sim[treatment == 0]), 2)` |
| **SMD** | **`r round(smd_age, 3)`**                   |

### Balance Thresholds

| SMD             | Interpretation                          |
|-----------------|-----------------------------------------|
| \|SMD\| \< 0.1  | Good balance (groups are similar)       |
| \|SMD\| \< 0.25 | Acceptable balance                      |
| \|SMD\| \> 0.25 | Poor balance (groups are too different) |

```{r balance-interpretation, echo=FALSE}
if (abs(smd_age) > 0.25) {
  cat("With SMD =", round(smd_age, 3), ", we have POOR balance.\n")
  cat("This means age is confounded with treatment.\n")
  cat("We need matching to fix this!")
} else if (abs(smd_age) > 0.1) {
  cat("With SMD =", round(smd_age, 3), ", we have ACCEPTABLE but not ideal balance.")
} else {
  cat("With SMD =", round(smd_age, 3), ", we have GOOD balance.")
}
```

------------------------------------------------------------------------

# Part 7: Visual Summary - What Different SMDs Look Like

```{r smd-visual, fig.height=6}
set.seed(42)
n_demo <- 500

smd_examples <- data.frame(
  value = c(
    rnorm(n_demo, mean = 0, sd = 1),    # Control (SMD = 0)
    rnorm(n_demo, mean = 0, sd = 1),    # Treatment group 1
    rnorm(n_demo, mean = 0, sd = 1),    # Control (SMD = 0.5)
    rnorm(n_demo, mean = 0.5, sd = 1),  # Treatment group 2
    rnorm(n_demo, mean = 0, sd = 1),    # Control (SMD = 1.0)
    rnorm(n_demo, mean = 1.0, sd = 1)   # Treatment group 3
  ),
  group = rep(c("Control", "Treated"), times = 3, each = n_demo),
  scenario = rep(c("SMD = 0 (Perfect Balance)",
                   "SMD = 0.5 (Medium Imbalance)",
                   "SMD = 1.0 (Severe Imbalance)"), each = 2 * n_demo)
)

ggplot(smd_examples, aes(x = value, fill = group)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ scenario, ncol = 1) +
  scale_fill_manual(values = c("Control" = "steelblue", "Treated" = "coral")) +
  labs(title = "What Different SMD Values Look Like",
       subtitle = "SMD measures how far apart the distributions are",
       x = "Value (standardized)", y = "Density",
       fill = "Group") +
  theme_minimal()
```

------------------------------------------------------------------------

# Part 8: SMD vs Statistical Significance

> **IMPORTANT:** SMD is NOT the same as statistical significance!

## 8.1 What SMD Actually Tells You

SMD tells you **HOW DIFFERENT** two groups are, measured in standard deviations.

Think of it as "overlap" between distributions:

| SMD | Overlap           | \% of Treated Above Control Mean |
|-----|-------------------|----------------------------------|
| 0.0 | Complete overlap  | 50%                              |
| 0.5 | Moderate overlap  | \~69%                            |
| 1.0 | Little overlap    | \~84%                            |
| 2.0 | Almost no overlap | \~97.7%                          |

## 8.2 The Key Difference: SMD vs P-values

|                         | SMD         | P-value                |
|-------------------------|-------------|------------------------|
| Depends on sample size? | **NO**      | YES                    |
| What it measures        | Effect SIZE | Probability under null |
| With n=10,000           | Same SMD    | Tiny p-value           |
| With n=10               | Same SMD    | Large p-value          |

**The relationship:** $$t\text{-statistic} \approx SMD \times \sqrt{\frac{n}{2}}$$

## 8.3 Demonstration: Same SMD, Different P-values

```{r same-smd-diff-p}
set.seed(999)

# Small sample
n_small <- 20
g1_small <- rnorm(n_small/2, mean = 0, sd = 1)
g2_small <- rnorm(n_small/2, mean = 0.5, sd = 1)
smd_small <- (mean(g2_small) - mean(g1_small)) / sqrt((sd(g1_small)^2 + sd(g2_small)^2) / 2)
ttest_small <- t.test(g2_small, g1_small)

# Medium sample
n_med <- 100
g1_med <- rnorm(n_med/2, mean = 0, sd = 1)
g2_med <- rnorm(n_med/2, mean = 0.5, sd = 1)
smd_med <- (mean(g2_med) - mean(g1_med)) / sqrt((sd(g1_med)^2 + sd(g2_med)^2) / 2)
ttest_med <- t.test(g2_med, g1_med)

# Large sample
n_large <- 1000
g1_large <- rnorm(n_large/2, mean = 0, sd = 1)
g2_large <- rnorm(n_large/2, mean = 0.5, sd = 1)
smd_large <- (mean(g2_large) - mean(g1_large)) / sqrt((sd(g1_large)^2 + sd(g2_large)^2) / 2)
ttest_large <- t.test(g2_large, g1_large)
```

| Sample Size | SMD | P-value | Significant? |
|----|----|----|----|
| n = 20 | `r round(smd_small, 3)` | `r round(ttest_small$p.value, 4)` | `r ifelse(ttest_small$p.value < 0.05, "YES", "NO")` |
| n = 100 | `r round(smd_med, 3)` | `r round(ttest_med$p.value, 4)` | `r ifelse(ttest_med$p.value < 0.05, "YES", "NO")` |
| n = 1000 | `r round(smd_large, 3)` | `r format(ttest_large$p.value, scientific = TRUE, digits = 2)` | `r ifelse(ttest_large$p.value < 0.05, "YES", "NO")` |

**CONCLUSION:** All three have similar SMD (\~0.5), but very different p-values! With large samples, EVERYTHING becomes "statistically significant".

```{r viz-same-smd, fig.height=6}
vizSmall <- data.frame(value = c(g1_small, g2_small),
                       group = rep(c("Control", "Treated"), each = n_small/2),
                       sample = "n=20")
vizMed <- data.frame(value = c(g1_med, g2_med),
                     group = rep(c("Control", "Treated"), each = n_med/2),
                     sample = "n=100")
vizLarge <- data.frame(value = c(g1_large[1:100], g2_large[1:100]),
                       group = rep(c("Control", "Treated"), each = 100),
                       sample = "n=1000")

vizData <- rbind(vizSmall, vizMed, vizLarge)
vizData$sample <- factor(vizData$sample, levels = c("n=20", "n=100", "n=1000"))

ggplot(vizData, aes(x = value, fill = group)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~ sample, ncol = 1) +
  scale_fill_manual(values = c("Control" = "steelblue", "Treated" = "coral")) +
  labs(title = "Same SMD (~0.5), Different P-values Due to Sample Size",
       subtitle = "Larger n → smaller p-value, but SMD stays the same",
       x = "Value", y = "Density") +
  theme_minimal()
```

## 8.4 Why We Use SMD for Balance (Not P-values)

In propensity score matching, we check BALANCE using **SMD**, not p-values.

**Why?**

1.  **P-values are confounded by sample size**
    -   Large studies show "significant" imbalance even for tiny differences
    -   Small studies might miss substantial imbalance
2.  **SMD tells you the PRACTICAL importance of imbalance**
    -   SMD = 0.01 is trivial, even if p \< 0.001 with large n
    -   SMD = 0.50 is substantial, even if p \> 0.05 with small n
3.  **SMD has intuitive thresholds that don't change with sample size**
    -   \|SMD\| \< 0.1: Good balance (groups practically identical)
    -   \|SMD\| \< 0.25: Acceptable balance
    -   \|SMD\| \> 0.25: Poor balance (confounding likely)

## 8.5 The Bottom Line

| Question                                  | Use This |
|-------------------------------------------|----------|
| "How different are these groups?"         | **SMD**  |
| "Could this difference be due to chance?" | P-value  |

For **BALANCE assessment** in matching:

-   ✅ **USE SMD** - it tells you if groups are practically similar
-   ❌ **DON'T rely on p-values** - they'll mislead you with large samples

------------------------------------------------------------------------

# Part 9: Can We Standardize Categorical Variables?

This is an important question! Not all variables can be standardized.

## 9.1 Types of Categorical Variables

Categorical variables come in two main types:

**1. NOMINAL (no natural order):**

-   Colors: red, blue, green
-   Country: USA, France, Japan
-   Gender: male, female, other
-   Major: economics, political science, engineering

**2. ORDINAL (natural order exists):**

-   Education: high school \< bachelor's \< master's \< PhD
-   Satisfaction: very dissatisfied \< dissatisfied \< neutral \< satisfied \< very satisfied
-   Income bracket: low \< medium \< high
-   Pain level: none \< mild \< moderate \< severe

## 9.2 Why NOMINAL Variables CANNOT Be Standardized

Standardization requires:

$$z = \frac{\text{value} - \text{mean}}{SD}$$

For this formula to make sense, you need:

1.  **Meaningful arithmetic** (can you subtract 'blue' from 'red'?)
2.  **A meaningful mean** (what's the 'average' of red, blue, green?)
3.  **A meaningful distance** (is blue '2 units' from red?)

**Example: Favorite color**

-   If we code: red=1, blue=2, green=3
-   Mean = 2 (blue?) - this is **MEANINGLESS!**
-   The numbers are just arbitrary labels
-   Changing to red=5, blue=10, green=15 would give different 'z-scores'

> **CONCLUSION:** You **CANNOT** standardize nominal variables. The numbers assigned to categories are arbitrary.

## 9.3 Can ORDINAL Variables Be Standardized?

Ordinal variables have **ORDER** but not equal **INTERVALS**.

**Example: Education level**

-   High school (1) \< Bachelor's (2) \< Master's (3) \< PhD (4)
-   We know 4 \> 3 \> 2 \> 1
-   But is the 'gap' from HS to Bachelor's the same as Master's to PhD?
-   Probably not! These intervals are NOT necessarily equal.

**The problem:**

-   Standardization assumes equal intervals
-   z = 1 means '1 SD above mean'
-   This assumes moving 1 unit anywhere on the scale means the same thing

**IN PRACTICE (with caution):**

-   Many researchers DO standardize ordinal variables
-   Especially with many categories (e.g., Likert 1-7)
-   It's an approximation that often works reasonably well
-   But be aware of the assumption being made!

## 9.4 What To Do Instead for Categorical Variables

**For NOMINAL variables in balance checking:**

-   Compare PROPORTIONS in each category between groups
-   Use chi-squared test for overall difference
-   Report percentage in each category by treatment group

**For BINARY variables (special case of nominal):**

SMD **CAN** be calculated for binary variables!

$$SMD_{binary} = \frac{p_1 - p_0}{\sqrt{p(1-p)}}$$

Where $p_1$, $p_0$ are proportions in each group and $p$ is the pooled proportion.

```{r binary-smd-example}
set.seed(456)
n_demo <- 200

# Create confounded treatment assignment based on a binary variable
is_stem <- rbinom(n_demo, 1, 0.4)  # 40% are STEM majors
treat_prob <- 0.3 + 0.4 * is_stem  # STEM more likely to be treated
treatment <- rbinom(n_demo, 1, treat_prob)

# Calculate proportions
p1 <- mean(is_stem[treatment == 1])  # Proportion STEM in treated
p0 <- mean(is_stem[treatment == 0])  # Proportion STEM in control
p_pooled <- mean(is_stem)

# SMD for binary variable
smd_binary <- (p1 - p0) / sqrt(p_pooled * (1 - p_pooled))

binary_balance <- data.frame(
  Group = c("Treated", "Control", "Difference"),
  Proportion_STEM = c(round(p1, 3), round(p0, 3), round(p1 - p0, 3)),
  SMD = c("-", "-", round(smd_binary, 3))
)
kable(binary_balance)
```

The SMD of `r round(smd_binary, 3)` tells us STEM majors are overrepresented in the treated group (imbalance that needs fixing).

## 9.5 Summary: What Can Be Standardized?

```{r standardization-summary-table}
summary_table <- data.frame(
  Variable_Type = c("Continuous", "Binary (0/1)", "Ordinal", "Nominal"),
  Can_Standardize = c("YES", "YES (modified)", "MAYBE (with caution)", "NO"),
  Notes = c("Standard z-score formula",
            "Use proportion-based SMD formula",
            "Assumes equal intervals (often violated)",
            "No meaningful arithmetic on categories")
)
kable(summary_table, col.names = c("Variable Type", "Can Standardize?", "Notes"))
```

**KEY TAKEAWAY:**

-   **Continuous & binary:** Standardize and use SMD
-   **Ordinal:** Can approximate, but interpret carefully
-   **Nominal:** Cannot standardize - compare proportions instead

------------------------------------------------------------------------

# Summary: Key Concepts

## 1. Standardization (Z-score)

-   Converts any variable to mean=0, SD=1
-   Formula: $z = \frac{\text{value} - \text{mean}}{SD}$
-   Allows comparing variables with different scales

## 2. Pooled Standard Deviation

-   Combines variability from two groups
-   Formula: $\text{pooled SD} = \sqrt{\frac{SD_1^2 + SD_2^2}{2}}$
-   Used when groups have different spreads

## 3. Standardized Mean Difference (SMD)

-   Measures group difference in SD units
-   Formula: $SMD = \frac{\text{mean}_1 - \text{mean}_2}{\text{pooled SD}}$
-   Thresholds: \<0.1 good, \<0.25 acceptable, \>0.25 poor

## 4. Why SMD Matters for Matching

-   Before matching: SMD shows imbalance (confounding)
-   After matching: SMD should decrease (balance improved)
-   Goal: \|SMD\| \< 0.1 for all covariates

## 5. SMD vs P-values

-   SMD is **effect size** (independent of sample size)
-   P-values depend heavily on sample size
-   Use SMD, not p-values, for balance assessment

## 6. Categorical Variables

-   **Nominal:** Cannot standardize (no meaningful arithmetic)
-   **Binary:** Can calculate SMD using proportion-based formula
-   **Ordinal:** Can approximate with caution (assumes equal intervals)
